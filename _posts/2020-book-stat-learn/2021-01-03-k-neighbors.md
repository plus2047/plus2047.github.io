---
layout: mypost
title: 统计学习方法 第 03 章 K 近邻法
categories: [mlearn]
---

K 近邻算法没有模型参数，其损失函数的意义与其他机器学习算法有所不同。K 近邻算法不使用损失函数更新参数，而是直接用损失函数进行分类。

## KD 算法

`kd` 树是在 `k` 维欧氏空间搜索任意个最近邻点的通用算法，当样本点数目 `N` 远大于维度数目时算法效率为 `log N`. 该算法类似 Segment Tree 数据结构

算法的第一个要点是建立关于空间的二叉搜索树。在样本点的高纬空间上任取一条坐标轴（可以按顺序依次取坐标轴），然后在这条坐标轴上取一个垂直于坐标轴的超平面，将样本点分为两等份。之后进行递归划分，从而将空间分隔为一个个超矩形，每个叶节点对应一个超矩形，样本点都在叶节点上（书中的算法在样本点个数为奇数时会有样本点在分割平面上，从编程实现的角度，这会增加麻烦，不妨规定样本点都在叶节点上。样本点为基数不能均分时，某个子节点多分一个样本点并不是什么大问题）。

第二个要点在于如何进行查找。建树时，应该保存好每个节点对应的矩形范围。查找某个新目标样本点时，可以根据根节点左右子节点的矩形范围进行递归查找，以对数速度找到该节点所属的矩形。然后选择该叶节点中的样本点作为候选最近临，则真实最近临位于目标样本点为中心，候选最近临与目标样本点距离为半径的超球体内。之后递归。空间划分树能够快速找到所有与超球体相交的叶节点，逐个检查这些叶节点对应的样本点是否为更优最近临即可（每检查一次，超球体都有可能缩小，从而使问题规模缩小）。

另外，可以参考 Locality Sensitive Hashing 局部敏感哈希，另一种在高维空间中高效搜索 K 近邻的算法，一个典型的应用是在推荐系统的召回问题。
