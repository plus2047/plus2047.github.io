---
layout: mypost
title: PySpark Notes
categories: [data-tools]
---

本笔记主要整理 Spark 高层设计思想，其技术细节查阅文件 PySpark CheatSheet。Spark 要解决的问题是 Hadoop 的 MapReduce 算法过于僵化，必须分为 Map 和 Reduce 两个步骤且两个步骤之间会存盘。与 Hadoop 一样，Spark 的基本思想也简单直观。

Spark 设计了更为灵活的 RDD(Resilient Distributed Datasets) 数据结构。从分布式文件系统中读取的数据被保存为 RDD，这基本上是个分布、容错的数据表，可以针对每一行进行特定操作，或者对整个表按照特定键值排序等。能够分布执行的操作 Spark 会尽量分布执行，而且 Spark 会惰性执行，直到必要时求解整个计算图。

Spark 中有很多函数式编程思想。RDD 是不可变数据结构，各种操作会形成新的 RDD. 最终处理结果保存到分布式文件系统或者对于较小结果直接打印输出即可。

-----

`pyspark` 可以解释运行，能够大大降低学习成本。

```
# ./spark-2.4.0-bin-hadoop2.7/bin/pyspark
# script:
from pyspark import SparkContext
logFile = "file:///Users/plus/workspace/learn_spark/spark-2.4.0-bin-hadoop2.7/README.md"
sc = SparkContext("local", "first app")
logData = sc.textFile(logFile).cache()
numAs = logData.filter(lambda s: 'a' in s).count()
numBs = logData.filter(lambda s: 'b' in s).count()
print("Lines with a: %i, lines with b: %i" % (numAs, numBs))
```

函数参考 PySpark CheatSheet (文件系统中搜索).